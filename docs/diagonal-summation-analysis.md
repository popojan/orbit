# Diagonal Summation: Layer-by-Layer Analysis of L_M(s)

**Date**: November 16, 2025, 22:30 CET
**Status**: â¸ï¸ EXPLORATION - Potential convergence improvement strategy

---

## Motivation

**Problem**: Current methods for computing L_M(s) on critical line Re(s) = 1/2:
- âŒ Closed form: oscillates, doesn't converge (tested Nov 16)
- âŒ Integral transform: slow convergence, 38% error at s=1.5
- âŒ Direct summation: 160% oscillation at s=0.5+5i

**Question**: Could **diagonal summation** (grouping by âˆšn layers) improve convergence?

**Inspiration**: Primal forest paper-cs describes natural stratification by âˆšn.

---

## Three Summation Orders in Primal Forest

For poles at (d,k) where dÂ²+kd â‰¤ n:

### 1. By Columns (fixed d)
```
Î£_d [Î£_{k: dÂ²+kdâ‰¤n} contribution(d,k)]
```
Sum over all k for each divisor d.

### 2. By Rows (fixed k)
```
Î£_k [Î£_{d: dÂ²+kdâ‰¤n} contribution(d,k)]
```
Sum over all d for each offset k.

### 3. By Diagonals (fixed n = dÂ²+kd) â­ **THIS PROPOSAL**
```
Î£_n [Î£_{d,k: dÂ²+kd=n} contribution(d,k)] / n^s
```
Sum over numbers n, collecting all poles on hyperbola dÂ²+kd = n.

---

## Diagonal = âˆšn Layer

**Key observation**: For fixed n, solving dÂ²+kd = n:
```
k = n/d - d
```

**Integer solutions**: k â‰¥ 1 requires:
- d | n  (d divides n)
- d â‰¤ âˆšn  (so that k = n/d - d â‰¥ 1)

**Therefore**: Diagonal for n contains exactly the divisors counted by M(n)!

**This is the geometric meaning**: M(n) = number of poles on hyperbola dÂ²+kd = n.

---

## Formalization: Layer-by-Layer Summation

**Definition**: Layer m = all integers n with âŒŠâˆšnâŒ‹ = m
```
Layer(m) = {n : mÂ² â‰¤ n < (m+1)Â²}
         = {mÂ², mÂ²+1, mÂ²+2, ..., mÂ²+2m}
```

**Size of layer**: |Layer(m)| = 2m + 1

**Layer-wise summation**:
```
L_M(s) = Î£_{m=1}^âˆ L_M^{(m)}(s)

where L_M^{(m)}(s) = Î£_{k=0}^{2m} M(mÂ²+k) / (mÂ²+k)^s
```

---

## Asymptotic Analysis of Layers

### Average M(n) in Layer m

From asymptotics (Question D, Web session):
```
M(n) ~ ln(n) / 2  as n â†’ âˆ
```

For n âˆˆ Layer(m):
```
M(n) ~ ln(mÂ²) / 2 = ln(m)
```

**Layer average**:
```
âŸ¨MâŸ©_m â‰ˆ ln(m)
```

### Layer Contribution to L_M(s)

**Rough estimate**:
```
L_M^{(m)}(s) â‰ˆ (2m+1) Â· ln(m) / (mÂ²)^s
            = O(m ln(m) / m^{2s})
            = O(ln(m) / m^{2s-1})
```

**Convergence**:
- Re(2s-1) > 0  âŸº  Re(s) > 1/2  âœ“ (includes critical line!)
- But coefficient ln(m) grows â†’ slow convergence

---

## Critical Line Behavior: s = 1/2 + it

**Layer contribution**:
```
L_M^{(m)}(1/2+it) â‰ˆ ln(m) / m^{it}
                   = ln(m) Â· e^{-it ln(m)}
                   = ln(m) Â· [cos(t ln m) - i sin(t ln m)]
```

**This is an oscillating series!**

**Key features**:
1. **Amplitude**: ln(m) grows (slow)
2. **Phase**: t ln(m) (frequency increases with m)
3. **Envelope decay**: 1 (borderline divergent without regularization)

**Standard issue**: Conditionally convergent, sensitive to summation order.

---

## Potential Advantages of Layer Summation

### 1. **Natural Regularization Points**

Each layer is **finite** (2m+1 terms). We can:
- Compute L_M^{(m)} exactly
- Apply regularization **per layer** instead of globally
- Use CesÃ ro/Abel summation on outer sum over m

### 2. **Connection to Poisson Summation**

For fixed m, sum over k:
```
Î£_{k=0}^{2m} f(mÂ²+k)
```

This is a **finite arithmetic sequence** â†’ could apply Euler-Maclaurin or Poisson.

### 3. **Acceleration via Layer Grouping**

**Idea**: Group consecutive layers:
```
L_M(s) = Î£_{M=1}^âˆ [Î£_{m=2^M}^{2^{M+1}} L_M^{(m)}(s)]
```

Binary grouping â†’ exponential growth â†’ better for acceleration algorithms.

### 4. **Mellin Transform Reinterpretation**

**Current Mellin**:
```
L_M(s) = âˆ«â‚€^âˆ t^{s-1} Î˜_M(t) dt
where Î˜_M(t) = Î£_{n=2}^âˆ M(n) e^{-nt}
```

**Layer-wise Mellin**:
```
L_M(s) = Î£_m âˆ«â‚€^âˆ t^{s-1} Î˜_M^{(m)}(t) dt
where Î˜_M^{(m)}(t) = Î£_{n âˆˆ Layer(m)} M(n) e^{-nt}
```

**Advantage**: Each Î˜_M^{(m)} is finite sum â†’ easier to compute integral exactly!

---

## Comparison: Layer vs Direct Summation

### Direct Summation
```
L_M(s) = Î£_{n=2}^N M(n)/n^s  (truncate at N)
```

**Convergence**: O(1/N^{Re(s)-1}) per term

**For s = 1/2+it**: O(1/âˆšN) â†’ need N ~ 10^6 for 3 digits

### Layer Summation
```
L_M(s) = Î£_{m=1}^M [exact layer sum]  (truncate at M layers)
```

**Layer sum**: Computed exactly (2M+1 terms)

**Outer convergence**: O(ln(M)/M^{2Re(s)-1})

**For s = 1/2+it**: O(ln(M)) oscillating â†’ **need regularization**

**But**: Could apply **Shanks/Wynn acceleration** on outer sum!

---

## Practical Implementation Strategy

### Step 1: Exact Layer Computation
```python
def layer_sum(m, s):
    """Compute L_M^{(m)}(s) exactly"""
    total = 0
    for k in range(2*m + 1):
        n = m**2 + k
        total += M(n) / n**s
    return total
```

### Step 2: Regularized Summation
```python
def L_M_layer_regularized(s, M_max):
    """Sum layers with regularization"""
    layers = [layer_sum(m, s) for m in range(1, M_max+1)]

    # Apply Wynn epsilon algorithm for acceleration
    return wynn_epsilon(layers)
```

### Step 3: Comparison Test

Test at s = 0.5 + 5i:
- Direct sum (N=1000): ?
- Layer sum (M=31, covers same N): ?
- Layer + Wynn acceleration: ?

**Hypothesis**: Acceleration on layers works better than on direct sum.

---

## Connection to Mellin Puzzle

**Observation**: Layer structure might explain (Î³-1) vs (2Î³-1)!

**Summatory function** (integral over layers):
```
Î£_{nâ‰¤x} M(n) = Î£_{mâ‰¤âˆšx} [Î£_{n âˆˆ Layer(m)} M(n)]
```

This **double sum** structure could introduce factor of 2 in residue!

**Mechanism**:
- Direct Mellin inversion picks up residue from **pole at s=1**
- But layer-by-layer, each layer contributes to both:
  - Double pole term (from integration)
  - Simple pole term (from layer structure)

**Factor 2**: Might come from **symmetric/antisymmetric splitting** of layers?

---

## Next Steps

### Immediate (Test Hypothesis)
1. Implement layer_sum() and compare convergence
2. Test Wynn acceleration on layers vs direct sum
3. Measure speedup at s = 1/2 + 5i, 1/2 + 14.135i

### Medium Term (Theory)
4. Derive **exact error bounds** for layer truncation
5. Prove (or disprove) that layer summation converges on critical line
6. Connection to **Poisson summation** on layers

### Long Term (Mellin Puzzle)
7. Formalize how layer structure relates to (Î³-1) vs (2Î³-1)
8. Write rigorous proof if connection holds
9. Publish if this resolves puzzle!

---

## Potential Outcomes

### Best Case Scenario ğŸ¯
- Layer summation + acceleration **works** on critical line
- Convergence 10-100Ã— faster than direct sum
- **Solves numerical access problem**
- Enables testing L_M at Riemann zeros!

### Moderate Success âœ…
- Marginal improvement (2-3Ã—)
- Theoretical insight into structure
- Helps explain Mellin puzzle

### Null Result âŒ
- No convergence improvement
- But still valuable perspective on structure
- Documents failed approach (important!)

---

## Relation to Primal Forest Paper

**From paper-cs** (paraphrasing):
> "Primal forest naturally stratifies by âˆšn layers. Each layer corresponds to a range of divisor sizes. The forest 'grows outward' from origin in concentric shells."

**Our formalization**: Make this **quantitative**!
- Layer m = shell at radius ~m
- M(n) for n âˆˆ Layer(m) counts poles in that shell
- L_M(s) = weighted sum over shells (weight = 1/n^s)

**Geometric picture**:
```
Layer 1:  n=1              (1 number)
Layer 2:  n=4,5            (2 numbers)
Layer 3:  n=9,10,11,12,13,14,15   (7 numbers)
...
Layer m:  n=mÂ²,...,mÂ²+2m   (2m+1 numbers)
```

Each layer forms a **âˆšn shell** in primal forest!

---

## Status

**Current**: ğŸ”¬ HYPOTHESIS - needs testing

**Implementation difficulty**: Easy (few hours)

**Potential impact**: High (could solve critical line access)

**Risk**: Low (worst case = null result + documentation)

**Recommendation**: â­ **IMPLEMENT AND TEST** - high reward/risk ratio!

---

**References**:
- Primal forest paper-cs: docs/papers/primal-forest-paper-cs.tex
- Mellin puzzle: docs/mellin-puzzle-resolution.md
- AC attempts summary: docs/STATUS.md Â§ Analytic Continuation Attempts
